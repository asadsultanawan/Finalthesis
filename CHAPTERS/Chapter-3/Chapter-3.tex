\chapter{Convolutional Neural Networks (CNNs)}
\label{Chapter 3}

CNNs are types of artificial neural networks used for object detection and image classification etc. Neural Networks are a brain-inspired set of algorithms expected to mimic the working of the human brain. Billions of neurons in our brain process information through electrical signals. Dendrites receive external information or stimuli in the form of electrical impulses, which are further processed in the cell body. The neuron integrates all the signals coming in, and the output is carried away to the downstream neurons through Axons \cite{chap_3_article:1}. The neuron chooses to either reject or accept the signal depending on the signal strength. Likewise, an artificial neural network has a large number of interconnected nodes called neurons. The computational node and cell body of neurons work in a similar way where we have a large number of signals coming as inputs to neurons. Each signal has a weight W associated with it and the 
summation of these weighted inputs is passed to the activation function where 
 non-linearity is introduced in the output. Neural networks can gradually 
update the weights of the interconnections of the neuron until desirable results 
are obtained during the training phase of the network.

\begin{figure}[H]
    \centering
        \includegraphics[width=0.70\textwidth]{CHAPTERS/Chapter-3/Images/3_1.jpg}
    \caption{Working of a single computational node}
    \label{fig:3.1}
\end{figure}

These neurons or computational nodes are organized into layers. The neurons in each preceding layer are interconnected to the neurons of the following layer. The input layer of the feed-forward neural networks feeds the information to the network, and mathematical computation is performed. The hidden layer, also known as the distillation layer, acts as a cell body and performs computation to extract salient features and patterns from the input data. Figure \ref{fig:3.2} illustrates a simple 
a feed-forward neural network having one 
hidden layer \cite{chap_3_article:2}.
The output layer provides the results for the given information. 

\begin{figure}[H]
    \centering
        \includegraphics[width=0.70\textwidth]{CHAPTERS/Chapter-3/Images/3.2.png}
    \caption{Simple Feed Forward Neural Network with one hidden layer}
    \label{fig:3.2}
\end{figure}

The pixels of the image are arranged in a particular way i.e., the appearance of the picture will be changed if we change the coordinate of one pixel. However, a simple feed-forward neural network cannot learn the inherent spatial relationships within images and fails to extract high-level features needed for image classification. CNNs are a class
of neural networks that learn visual filters to recognize higher-level image features, and the spatial information is preserved. As a result, CNN ends up learning this hierarchy of filters where filters at the early stage usually represent low-level features like edge detection, gradient orientation, and filters in the later stages representing more complex features specific to the image.

\section{Convolutional Neural Networks Architecture}

There are three types of layers associated with CNN. These layers include convolutional layers, pooling layers, and fully connected layers. When we stack together these layers, we get a CNN architecture. A general
architecture of CNNs with two convolutional layers is shown in \ref{fig:3.3} \cite{chap_3_article:2}.

\begin{figure}[H]
    \centering
        \includegraphics[width=0.80\textwidth]{CHAPTERS/Chapter-3/Images/3.3}
    \caption{Convolutional Neural Network Architecture with two convolutional layers}
    \label{fig:3.3}
\end{figure}

\subsection{Convolutional Layer}
Convolutional layers are an integral part of CNNs, and when these layers are stacked
together, we start extracting more simple features. These features are aggregated into more complex features later on. There is a set of learnable filters called kernels with
height and width smaller than the input image. These filters extend to the full depth of the input volume. We slide the filter over the entire image to perform the convolution operation, i.e., multiplying each element of the input image with the filter weights. The resultants are summed up, and the value is assigned to the top left corner of the receptive field, as shown in figure 3.4 \cite{chap_3_article:4}.

\begin{figure}[H]
    \centering
        \includegraphics[width=0.70\textwidth]{CHAPTERS/Chapter-3/Images/3.4}
    \caption{3x3 filter with stride value of 1}
    \label{fig:3.4}
\end{figure}

The kernel window slides right or downward by a particular value known as stride value in each step, and we repeat the process
until the whole image is processed. Stride value of
1 moves the filter 1 pixel in each step. This results in a feature map with a much smaller size than the input image. The size of the resultant feature map keeps shrinking at every layer, which may not be desirable. However, the dimensionality of the input image can be preserved by padding zero-pixel values across every side of the image.

\begin{figure}[H]
    \centering
        \includegraphics[width=0.70\textwidth]{CHAPTERS/Chapter-3/Images/3.5}
    \caption{Result  of 3x3 filter of Stride 1 with padding}
    \label{fig:3.5}
\end{figure}

In reality, we apply multiple filters to the same 
the input image and feature maps are obtained depending upon the 
the number of filters. In this case the output of the convolutional layer 
is represented with a 3D matrix with dimensions of height, width 
and depth, where the depth 
shows the no. of filters applied to the image. 

\begin{figure}[H]
    \centering
        \includegraphics[width=0.70\textwidth]{CHAPTERS/Chapter-3/Images/3.6}
    \caption{Multiple filters applied on one input}
    \label{fig:3.6}
\end{figure}

\subsubsection{Activation Functions}

An activation function is a mathematical function applied to every neuron present in the network and decides whether to activate the neuron or not depending on how much neuron’s input is relevant to the model’s prediction. 
In this way, a non-linearity is introduced in the output with the help of the activation function. Without the activation function, our network will be reduced to a simple regression model that would be unable to model and learn complex and complicated forms of data. Thus, non-linear functions make neural networks capable of performing non-linear transformations to learn complex kinds of data such as images, videos, etc. A good activation 
function should have the following properties:
\begin{itemize}
\item It should be zero-centered because when backpropagating gradient descent, the resulting values of the gradient will be all positive or negative. Thus, it will affect the gradient-based optimization process as we can only follow the zig-zag path to reach the optimal point.
\item It shouldn’t kill the gradient flow i.e., neurons get saturated on the boundaries of activation, and the local gradient comes out to be zero. As a result, it nullifies the gradient flow during backpropagation, and dead neurons will stop updating.
\item It should be computationally efficient as the activation function will be computed across millions of neurons for each input. Moreover, the backpropagation technique also puts a constraint on the choice of activation function i.e.; it should be differentiable.

\end{itemize}

\paragraph*{Sigmoid Function:}
A sigmoid function is represented by the mathematical function:
\begin{equation}
    sig(z) = \frac{1}{1+e^{-z}}
\end{equation}
This function takes input a real value and squeezes it between 0 and -1. It is often called
a squashing function as the large negative and positive values become 0 and 1, respectively \cite{chap_3_article:5}. But the sigmoid function output is not zero-centered, and it also kills the neurons as function gets saturated at higher values. That’s why a sigmoid function is usually not preferred in neural networks because of these drawbacks.

\begin{figure}[H]
    \centering
        \includegraphics[width=0.70\textwidth]{CHAPTERS/Chapter-3/Images/3.7.jpg}
    \caption{Sigmoid Function}
    \label{fig:3.7}
\end{figure}
\paragraph*{Tanh Function:}
A tanh sigmoid function is represented by the following mathematical function:
\begin{equation}
    tanh(z) = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
\end{equation}
It takes the real input value and squashes it between -1 and 1 i.e. very 
large negative and positive values are mapped into -1 and 1 respectively \cite{chap_3_article:5}. As 
a result, this non-linear activation function gives zero-mean data, but it 
still gets saturated at higher values as shown in \ref{fig:3.8}. However, tanh 
activations functions are preferred over sigmoid functions in neural networks.

\begin{figure}[H]
    \centering
        \includegraphics[width=0.70\textwidth]{CHAPTERS/Chapter-3/Images/3.8.jpg}
    \caption{Tanh function}
    \label{fig:3.8}
\end{figure}

\paragraph*{ReLU Function:}
ReLU stands for Rectified Linear unit and is represented by the following equation:

\begin{equation}
    ReLU(z) = max(0,z)
\end{equation}
It takes real values as an input and sets all the negative values equal to zero \cite{chap_3_article:3}. It is computationally very efficient as no complicated math need to be performed. And it is the most frequently used the activation function in neural networks as it increases the convergence rate of gradient descent by six times as compared to sigmoid and tanh activation 
functions. However, the function doesn’t get saturated for larger values of inputs. Still, the neurons often stick in a negative side, giving zero local gradient and neurons never getting a chance to update in gradient descent learning process. 
This problem is also known as dying ReLU, and it often occurs due to a high learning rate or a significant negative bias. Leaky ReLU can be used to avoid this situation. Leaky ReLU is very similar to the original ReLU. The only difference is that now instead of being flat in the negative regime, we are going to give a slight negative slope here. This solves a lot of problems we mentioned earlier
as it doesn’t saturate in the negative space and still very computationally efficient \cite{chap_3_article:3}.

% \begin{figure}
%   \centering
%   \begin{subfigure}{.5\textwidth}
%     \centering
%     \includegraphics[width=.4\linewidth]{CHAPTERS/Chapter-3/Images/3.9a}
%     \caption{A subfigure}
%     \label{fig:3.9(a)}
%   \end{subfigure}%
%   \begin{subfigure}{.5\textwidth}
%     \centering
%     \includegraphics[width=.4\linewidth]{CHAPTERS/Chapter-3/Images/3.9b}
%     \caption{A subfigure}
%     \label{fig:3.9(b)}
%   \end{subfigure}
%   \caption{ReLU and Leaky ReLU Function}
%   \label{fig:test}
% \end{figure}

\begin{figure}%
    \centering
    \subfloat[\label{fig:3.9(a)}]{{\includegraphics[height = 4.38cm, width=5cm]{CHAPTERS/Chapter-3/Images/3.9a} }}%
    \qquad
    \subfloat[\label{fig:3.9(b)}]{{\includegraphics[height = 4cm, width=5cm]{CHAPTERS/Chapter-3/Images/3.9b} }}%
    \caption{ReLU and Leaky ReLU Function}%
    \label{fig:3.9}%
\end{figure}

\subsection{Pooling Layer}
The pooling layer follows the non-linear function introduced in the convolutional layer in CNN. It simply downsamples the feature map to speed up the computation and makes some of the detected features a bit more robust \cite{chap_3_article:4}. The rest of the operations will be performed on the summarized features so that the feature map is no more sensitive to the precise location of features in the input. Two most 
commonly used pooling techniques are described below:

\subsubsection{Average Pooling}

In average pooling, we place the filter window and find the average of the pixel values covered by the filter on the feature map. We keep on sliding the filter and repeat the process until the entire input image is processed. The average pooling 
technique with 2 x 2 filter size and 
stride value of 2 is illustrated in \ref{fig:3.10(a)}.

\subsubsection{Max Pooling}
It is a pooling technique in which the most prominent features in the region of feature map are retained. We simply 
take the maximum value present under the filter and process the entire 
feature map to get a down-sampled output as shown in \ref{fig:3.10(b)} \cite{chap_3_article:6}.

\begin{figure}%
    \centering
    \subfloat[\label{fig:3.10(a)}]{{\includegraphics[height = 4cm, width=6cm]{CHAPTERS/Chapter-3/Images/3.10a} }}%
    \qquad
    \subfloat[\label{fig:3.10(b)}]{{\includegraphics[height = 4cm, width=6cm]{CHAPTERS/Chapter-3/Images/3.10b} }}%
    \caption{Average and Max Pooling}%
    \label{fig:3.10}%
\end{figure}

\subsection{Flatten Layer}

The flatter layer acts as a bridge between the last convolutional or pooling layer and fully connected layer. The function of the flatten layer is to take
a two-dimensional feature map and stretch it into a long column vector acting as an input to the fully connected layer of artificial neural network.


\begin{figure}[H]
    \centering
        \includegraphics[width=0.60\textwidth]{CHAPTERS/Chapter-3/Images/3.11}
    \caption{Flatten layer}
    \label{fig:3.11}
\end{figure}


\subsection{Fully Connected Layer} 

A fully connected layer is an important part of CNNs and gives the final probabilities for each class.  The output feature value vector obtained by the flatting layer is given as input to the fully connected layer. It classifies the input image into various classes using these features. The output is simply a vector of m $\times$ 1 dimension giving the probability of the classification label model is trying to predict. The probabilities of the
classes are obtained using the Softmax activation function, which squashes the input values between 0 and 1 that sums to one.

Thus we conclude that CNNs are divided into two separate categories i.e., feature learning and classification. In the feature learning part,
 data is passed through the convolutional layer, activation function, and pooling layer repeatedly to extract several features from the images. The resulting feature matrix is passed to the classification
layer. It is flattened to a single vector and generates probability for each class as an output of a fully connected layer.